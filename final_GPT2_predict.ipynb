{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_GPT2_predict.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EDbJEHgn4-wX",
        "outputId": "d25d3927-0c9a-4575-d59c-9b5ccfeac08d"
      },
      "source": [
        "! pip install tensorflow==1.15"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 33kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 47.7MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (54.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=ec5d278494a96daff7b63305769bdef8cad2b6ee106fdc9efd756665d119d4e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, gast, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtTza37o3iYr"
      },
      "source": [
        "\"\"\"Byte pair encoding utilities\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import regex as re\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "        self.errors = errors # how to handle errors in decoding\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "\n",
        "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
        "        return text\n",
        "\n",
        "def get_encoder(model_name, models_dir):\n",
        "    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "    return Encoder(\n",
        "        encoder=encoder,\n",
        "        bpe_merges=bpe_merges,\n",
        "    )\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWtsUVtf45y1"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.training import HParams\n",
        "\n",
        "def default_hparams():\n",
        "    return HParams(\n",
        "        n_vocab=0,\n",
        "        n_ctx=1024,\n",
        "        n_embd=768,\n",
        "        n_head=12,\n",
        "        n_layer=12,\n",
        "    )\n",
        "\n",
        "def shape_list(x):\n",
        "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
        "    static = x.shape.as_list()\n",
        "    dynamic = tf.shape(x)\n",
        "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n",
        "    ex = tf.exp(x)\n",
        "    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
        "\n",
        "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
        "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
        "    with tf.variable_scope(scope):\n",
        "        n_state = x.shape[-1].value\n",
        "        g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n",
        "        b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n",
        "        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
        "        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
        "        x = (x - u) * tf.rsqrt(s + epsilon)\n",
        "        x = x*g + b\n",
        "        return x\n",
        "\n",
        "def split_states(x, n):\n",
        "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
        "    *start, m = shape_list(x)\n",
        "    return tf.reshape(x, start + [n, m//n])\n",
        "\n",
        "def merge_states(x):\n",
        "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
        "    *start, a, b = shape_list(x)\n",
        "    return tf.reshape(x, start + [a*b])\n",
        "\n",
        "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
        "    with tf.variable_scope(scope):\n",
        "        *start, nx = shape_list(x)\n",
        "        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n",
        "        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n",
        "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
        "        return c\n",
        "\n",
        "def attention_mask(nd, ns, *, dtype):\n",
        "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
        "\n",
        "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
        "    \"\"\"\n",
        "    i = tf.range(nd)[:,None]\n",
        "    j = tf.range(ns)\n",
        "    m = i >= j - ns + nd\n",
        "    return tf.cast(m, dtype)\n",
        "\n",
        "\n",
        "def attn(x, scope, n_state, *, past, hparams):\n",
        "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
        "    assert n_state % hparams.n_head == 0\n",
        "    if past is not None:\n",
        "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
        "\n",
        "    def split_heads(x):\n",
        "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
        "        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n",
        "\n",
        "    def merge_heads(x):\n",
        "        # Reverse of split_heads\n",
        "        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
        "\n",
        "    def mask_attn_weights(w):\n",
        "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
        "        _, _, nd, ns = shape_list(w)\n",
        "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
        "        b = tf.reshape(b, [1, 1, nd, ns])\n",
        "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
        "        return w\n",
        "\n",
        "    def multihead_attn(q, k, v):\n",
        "        # q, k, v have shape [batch, heads, sequence, features]\n",
        "        w = tf.matmul(q, k, transpose_b=True)\n",
        "        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n",
        "\n",
        "        w = mask_attn_weights(w)\n",
        "        w = softmax(w)\n",
        "        a = tf.matmul(w, v)\n",
        "        return a\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "        c = conv1d(x, 'c_attn', n_state*3)\n",
        "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
        "        present = tf.stack([k, v], axis=1)\n",
        "        if past is not None:\n",
        "            pk, pv = tf.unstack(past, axis=1)\n",
        "            k = tf.concat([pk, k], axis=-2)\n",
        "            v = tf.concat([pv, v], axis=-2)\n",
        "        a = multihead_attn(q, k, v)\n",
        "        a = merge_heads(a)\n",
        "        a = conv1d(a, 'c_proj', n_state)\n",
        "        return a, present\n",
        "\n",
        "\n",
        "def mlp(x, scope, n_state, *, hparams):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = x.shape[-1].value\n",
        "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
        "        h2 = conv1d(h, 'c_proj', nx)\n",
        "        return h2\n",
        "\n",
        "\n",
        "def block(x, scope, *, past, hparams):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = x.shape[-1].value\n",
        "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
        "        x = x + a\n",
        "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
        "        x = x + m\n",
        "        return x, present\n",
        "\n",
        "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
        "    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
        "\n",
        "def expand_tile(value, size):\n",
        "    \"\"\"Add a new axis of given size.\"\"\"\n",
        "    value = tf.convert_to_tensor(value, name='value')\n",
        "    ndims = value.shape.ndims\n",
        "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
        "\n",
        "def positions_for(tokens, past_length):\n",
        "    batch_size = tf.shape(tokens)[0]\n",
        "    nsteps = tf.shape(tokens)[1]\n",
        "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
        "\n",
        "\n",
        "def model(hparams, X, past=None, scope='model', reuse=False):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        results = {}\n",
        "        batch, sequence = shape_list(X)\n",
        "\n",
        "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
        "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
        "\n",
        "        # Transformer\n",
        "        presents = []\n",
        "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
        "        assert len(pasts) == hparams.n_layer\n",
        "        for layer, past in enumerate(pasts):\n",
        "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
        "            presents.append(present)\n",
        "        results['present'] = tf.stack(presents, axis=1)\n",
        "        h = norm(h, 'ln_f')\n",
        "\n",
        "        # Language model loss.  Do tokens <n predict token n?\n",
        "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
        "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
        "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
        "        results['logits'] = logits\n",
        "        return results\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxjpLPj5dJ9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# import model\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    if k == 0:\n",
        "        # no truncation\n",
        "        return logits\n",
        "\n",
        "    def _top_k():\n",
        "        values, _ = tf.nn.top_k(logits, k=k)\n",
        "        min_values = values[:, -1, tf.newaxis]\n",
        "        return tf.where(\n",
        "            logits < min_values,\n",
        "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
        "            logits,\n",
        "        )\n",
        "    return tf.cond(\n",
        "       tf.equal(k, 0),\n",
        "       lambda: logits,\n",
        "       lambda: _top_k(),\n",
        "    )\n",
        "\n",
        "\n",
        "def top_p_logits(logits, p):\n",
        "    \"\"\"Nucleus sampling\"\"\"\n",
        "    batch, _ = logits.shape.as_list()\n",
        "    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)\n",
        "    cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
        "    indices = tf.stack([\n",
        "        tf.range(0, batch),\n",
        "        # number of indices to include\n",
        "        tf.maximum(tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 0),\n",
        "    ], axis=-1)\n",
        "    min_values = tf.gather_nd(sorted_logits, indices)\n",
        "    return tf.where(\n",
        "        logits < min_values,\n",
        "        tf.ones_like(logits) * -1e10,\n",
        "        logits,\n",
        "    )\n",
        "\n",
        "\n",
        "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, top_p=1):\n",
        "    if start_token is None:\n",
        "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
        "    else:\n",
        "        assert context is None, 'Specify exactly one of start_token and context!'\n",
        "        context = tf.fill([batch_size, 1], start_token)\n",
        "\n",
        "    def step(hparams, tokens, past=None):\n",
        "        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
        "        presents = lm_output['present']\n",
        "        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'presents': presents,\n",
        "        }\n",
        "\n",
        "    with tf.name_scope('sample_sequence'):\n",
        "        def body(past, prev, output):\n",
        "            next_outputs = step(hparams, prev, past=past)\n",
        "            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n",
        "            logits = top_k_logits(logits, k=top_k)\n",
        "            logits = top_p_logits(logits, p=top_p)\n",
        "            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n",
        "            return [\n",
        "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
        "                samples,\n",
        "                tf.concat([output, samples], axis=1)\n",
        "            ]\n",
        "\n",
        "        past, prev, output = body(None, context, context)\n",
        "\n",
        "        def cond(*args):\n",
        "            return True\n",
        "\n",
        "        _, _, tokens = tf.while_loop(\n",
        "            cond=cond, body=body,\n",
        "            maximum_iterations=length - 1,\n",
        "            loop_vars=[\n",
        "                past,\n",
        "                prev,\n",
        "                output\n",
        "            ],\n",
        "            shape_invariants=[\n",
        "                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "            ],\n",
        "            back_prop=False,\n",
        "        )\n",
        "\n",
        "        return tokens\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq-Gaogp5q9M"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# import model\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    if k == 0:\n",
        "        # no truncation\n",
        "        return logits\n",
        "\n",
        "    def _top_k():\n",
        "        values, _ = tf.nn.top_k(logits, k=k)\n",
        "        min_values = values[:, -1, tf.newaxis]\n",
        "        return tf.where(\n",
        "            logits < min_values,\n",
        "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
        "            logits,\n",
        "        )\n",
        "    return tf.cond(\n",
        "       tf.equal(k, 0),\n",
        "       lambda: logits,\n",
        "       lambda: _top_k(),\n",
        "    )\n",
        "\n",
        "\n",
        "def top_p_logits(logits, p):\n",
        "    \"\"\"Nucleus sampling\"\"\"\n",
        "    batch, _ = logits.shape.as_list()\n",
        "    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)\n",
        "    cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
        "    indices = tf.stack([\n",
        "        tf.range(0, batch),\n",
        "        # number of indices to include\n",
        "        tf.maximum(tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 0),\n",
        "    ], axis=-1)\n",
        "    min_values = tf.gather_nd(sorted_logits, indices)\n",
        "    return tf.where(\n",
        "        logits < min_values,\n",
        "        tf.ones_like(logits) * -1e10,\n",
        "        logits,\n",
        "    )\n",
        "\n",
        "\n",
        "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, top_p=1):\n",
        "    if start_token is None:\n",
        "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
        "    else:\n",
        "        assert context is None, 'Specify exactly one of start_token and context!'\n",
        "        context = tf.fill([batch_size, 1], start_token)\n",
        "\n",
        "    def step(hparams, tokens, past=None):\n",
        "        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
        "        presents = lm_output['present']\n",
        "        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'presents': presents,\n",
        "        }\n",
        "\n",
        "    with tf.name_scope('sample_sequence'):\n",
        "        def body(past, prev, output):\n",
        "            next_outputs = step(hparams, prev, past=past)\n",
        "            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n",
        "            logits = top_k_logits(logits, k=top_k)\n",
        "            logits = top_p_logits(logits, p=top_p)\n",
        "            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n",
        "            return [\n",
        "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
        "                samples,\n",
        "                tf.concat([output, samples], axis=1)\n",
        "            ]\n",
        "\n",
        "        past, prev, output = body(None, context, context)\n",
        "\n",
        "        def cond(*args):\n",
        "            return True\n",
        "\n",
        "        _, _, tokens = tf.while_loop(\n",
        "            cond=cond, body=body,\n",
        "            maximum_iterations=length - 1,\n",
        "            loop_vars=[\n",
        "                past,\n",
        "                prev,\n",
        "                output\n",
        "            ],\n",
        "            shape_invariants=[\n",
        "                tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "            ],\n",
        "            back_prop=False,\n",
        "        )\n",
        "\n",
        "        return tokens\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmG1IDyJ5tu3",
        "outputId": "4c81d64f-a2bc-41fd-a46b-389abe53d807"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = '124M'\n",
        "\n",
        "subdir = os.path.join('models', model)\n",
        "if not os.path.exists(subdir):\n",
        "    os.makedirs(subdir)\n",
        "subdir = subdir.replace('\\\\','/') # needed for Windows\n",
        "\n",
        "for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
        "\n",
        "    r = requests.get(\"https://openaipublic.blob.core.windows.net/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n",
        "\n",
        "    with open(os.path.join(subdir, filename), 'wb') as f:\n",
        "        file_size = int(r.headers[\"content-length\"])\n",
        "        chunk_size = 1000\n",
        "        with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
        "            # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
        "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                f.write(chunk)\n",
        "                pbar.update(chunk_size)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 221kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.43Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 299kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:36, 13.6Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 1.93Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 1.84Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.79Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LureHNY6CxP"
      },
      "source": [
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# import model, sample, encoder\n",
        "\n",
        "def interact_model(\n",
        "    model_name='774M',\n",
        "    seed=None,\n",
        "    nsamples=10,\n",
        "    batch_size=1,\n",
        "    length=None,\n",
        "    temperature=1,\n",
        "    top_k=40,\n",
        "    top_p=1,\n",
        "    models_dir='models',\n",
        "):\n",
        "    \"\"\"\n",
        "    Interactively run the model\n",
        "    :model_name=124M : String, which model to use\n",
        "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "     results\n",
        "    :nsamples=1 : Number of samples to return total\n",
        "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "    :length=None : Number of tokens in generated text, if None (default), is\n",
        "     determined by model hyperparameters\n",
        "    :temperature=1 : Float value controlling randomness in boltzmann\n",
        "     distribution. Lower temperature results in less random completions. As the\n",
        "     temperature approaches zero, the model will become deterministic and\n",
        "     repetitive. Higher temperature results in more random completions.\n",
        "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "     considered for each step (token), resulting in deterministic completions,\n",
        "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "     special setting meaning no restrictions. 40 generally is a good value.\n",
        "     :models_dir : path to parent folder containing model subfolders\n",
        "     (i.e. contains the <model_name> folder)\n",
        "    \"\"\"\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = get_encoder(model_name, models_dir)\n",
        "    hparams = default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k, top_p=top_p\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wdHT2nOk6eCZ",
        "outputId": "222acde2-f62c-4b27-a34d-5137afc49a45"
      },
      "source": [
        "interact_model(model_name='124M')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-7977ed40e49d>:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From <ipython-input-11-7977ed40e49d>:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-11-7977ed40e49d>:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Model prompt >>> 3.141\n",
            "======================================== SAMPLE 1 ========================================\n",
            ".0.42\n",
            "\n",
            "(c))\n",
            "\n",
            "| (m) (z) (y) (w)\n",
            "\n",
            "' (y (c) s)\n",
            "\n",
            "{\n",
            "\n",
            "#define CURRENT_CODE\n",
            "\n",
            "Z(x) // \\\n",
            "\n",
            "R(y) // \\\n",
            "\n",
            "R(z) // \\\n",
            "\n",
            "R(t) // \\\n",
            "\n",
            "L(t/p) // \\\n",
            "\n",
            "M(k) = 1 // \\\n",
            "\n",
            "R_1_l\n",
            "\n",
            "(f (x y) Z )\n",
            "\n",
            ". (z z)\n",
            "\n",
            ". (l z)\n",
            "\n",
            "}\n",
            "\n",
            "}\n",
            "\n",
            "}\n",
            "\n",
            "else if (z)\n",
            "\n",
            "{\n",
            "\n",
            "z. (d(w w/z))\n",
            "\n",
            "}\n",
            "\n",
            "else z.\n",
            "\n",
            "{\n",
            "\n",
            "z(z u) }\n",
            "\n",
            ";\n",
            "\n",
            "'((y z) -> z u)\n",
            "\n",
            "'(y z u z)\n",
            "\n",
            "'(x z u z U) // y u u u u u z u o u u u u r z u .\n",
            "\n",
            "#define CURRENT_CODE (z) // \\\n",
            "\n",
            "R(z u) // \\\n",
            "\n",
            "R(z u) // \\\n",
            "\n",
            "R(z u) // \\\n",
            "\n",
            "L(z u o u) // \\\n",
            "\n",
            "M(z u u o u z u z u u u z u u z u z u u\n",
            "\n",
            "; )\n",
            "\n",
            "}\n",
            "\n",
            "else\n",
            "\n",
            "{\n",
            "\n",
            "z.(x y\n",
            "\n",
            ". (y z u v)\n",
            "\n",
            ".(x z u v u)\n",
            "\n",
            ". (z u v u z u u u z u u z u u v u z u u u z u - z u v z u z z z u u _ .\n",
            "\n",
            "z U v z u z u v u z u u z u z u u v z u u z u z u u v u z z. u u z z U u Z z v z m z u o u\n",
            "\n",
            ". y u u z z u v z u u z z u u x u u z u u u u r - u z x o u s z o u u u s u u v u z z z u z u u z U z z\n",
            "\n",
            ". x z v u z u u z u z u z u z u u n u g u z z u u z z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-66b6442b254a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minteract_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'124M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-d81923858301>\u001b[0m in \u001b[0;36minteract_model\u001b[0;34m(model_name, seed, nsamples, batch_size, length, temperature, top_k, top_p, models_dir)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsamples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 out = sess.run(output, feed_dict={\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontext_tokens\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 })[:, len(context_tokens):]\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}